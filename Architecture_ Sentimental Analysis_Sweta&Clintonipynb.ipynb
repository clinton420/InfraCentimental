{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cd9a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "896ea7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 12:30:00</td>\n",
       "      <td>User123</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Nature #Park</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2023-01-15 08:45:00</td>\n",
       "      <td>CommuterX</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Traffic #Morning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Just finished an amazing workout! ðŸ’ª          ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 15:45:00</td>\n",
       "      <td>FitnessFan</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Fitness #Workout</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 18:20:00</td>\n",
       "      <td>AdventureX</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#Travel #Adventure</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2023-01-15 19:55:00</td>\n",
       "      <td>ChefCook</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Cooking #Food</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>728</td>\n",
       "      <td>732</td>\n",
       "      <td>Collaborating on a science project that receiv...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2017-08-18 18:20:00</td>\n",
       "      <td>ScienceProjectSuccessHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#ScienceFairWinner #HighSchoolScience</td>\n",
       "      <td>20.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>729</td>\n",
       "      <td>733</td>\n",
       "      <td>Attending a surprise birthday party organized ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2018-06-22 14:15:00</td>\n",
       "      <td>BirthdayPartyJoyHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#SurpriseCelebration #HighSchoolFriendship</td>\n",
       "      <td>25.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>730</td>\n",
       "      <td>734</td>\n",
       "      <td>Successfully fundraising for a school charity ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2019-04-05 17:30:00</td>\n",
       "      <td>CharityFundraisingTriumphHighSchool</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#CommunityGiving #HighSchoolPhilanthropy</td>\n",
       "      <td>22.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>731</td>\n",
       "      <td>735</td>\n",
       "      <td>Participating in a multicultural festival, cel...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2020-02-29 20:45:00</td>\n",
       "      <td>MulticulturalFestivalJoyHighSchool</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#CulturalCelebration #HighSchoolUnity</td>\n",
       "      <td>21.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>732</td>\n",
       "      <td>736</td>\n",
       "      <td>Organizing a virtual talent show during challe...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2020-11-15 15:15:00</td>\n",
       "      <td>VirtualTalentShowSuccessHighSchool</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#VirtualEntertainment #HighSchoolPositivity</td>\n",
       "      <td>24.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0  \\\n",
       "0               0           0   \n",
       "1               1           1   \n",
       "2               2           2   \n",
       "3               3           3   \n",
       "4               4           4   \n",
       "..            ...         ...   \n",
       "727           728         732   \n",
       "728           729         733   \n",
       "729           730         734   \n",
       "730           731         735   \n",
       "731           732         736   \n",
       "\n",
       "                                                  Text    Sentiment  \\\n",
       "0     Enjoying a beautiful day at the park!        ...   Positive     \n",
       "1     Traffic was terrible this morning.           ...   Negative     \n",
       "2     Just finished an amazing workout! ðŸ’ª          ...   Positive     \n",
       "3     Excited about the upcoming weekend getaway!  ...   Positive     \n",
       "4     Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
       "..                                                 ...          ...   \n",
       "727  Collaborating on a science project that receiv...       Happy    \n",
       "728  Attending a surprise birthday party organized ...       Happy    \n",
       "729  Successfully fundraising for a school charity ...       Happy    \n",
       "730  Participating in a multicultural festival, cel...       Happy    \n",
       "731  Organizing a virtual talent show during challe...       Happy    \n",
       "\n",
       "               Timestamp                                   User     Platform  \\\n",
       "0    2023-01-15 12:30:00                          User123          Twitter     \n",
       "1    2023-01-15 08:45:00                          CommuterX        Twitter     \n",
       "2    2023-01-15 15:45:00                          FitnessFan      Instagram    \n",
       "3    2023-01-15 18:20:00                          AdventureX       Facebook    \n",
       "4    2023-01-15 19:55:00                          ChefCook        Instagram    \n",
       "..                   ...                                    ...          ...   \n",
       "727  2017-08-18 18:20:00       ScienceProjectSuccessHighSchool     Facebook    \n",
       "728  2018-06-22 14:15:00            BirthdayPartyJoyHighSchool    Instagram    \n",
       "729  2019-04-05 17:30:00   CharityFundraisingTriumphHighSchool      Twitter    \n",
       "730  2020-02-29 20:45:00    MulticulturalFestivalJoyHighSchool     Facebook    \n",
       "731  2020-11-15 15:15:00    VirtualTalentShowSuccessHighSchool    Instagram    \n",
       "\n",
       "                                          Hashtags  Retweets  Likes  \\\n",
       "0        #Nature #Park                                  15.0   30.0   \n",
       "1        #Traffic #Morning                               5.0   10.0   \n",
       "2        #Fitness #Workout                              20.0   40.0   \n",
       "3        #Travel #Adventure                              8.0   15.0   \n",
       "4        #Cooking #Food                                 12.0   25.0   \n",
       "..                                             ...       ...    ...   \n",
       "727         #ScienceFairWinner #HighSchoolScience       20.0   39.0   \n",
       "728    #SurpriseCelebration #HighSchoolFriendship       25.0   48.0   \n",
       "729      #CommunityGiving #HighSchoolPhilanthropy       22.0   42.0   \n",
       "730         #CulturalCelebration #HighSchoolUnity       21.0   43.0   \n",
       "731   #VirtualEntertainment #HighSchoolPositivity       24.0   47.0   \n",
       "\n",
       "          Country  Year  Month  Day  Hour  \n",
       "0       USA        2023      1   15    12  \n",
       "1       Canada     2023      1   15     8  \n",
       "2     USA          2023      1   15    15  \n",
       "3       UK         2023      1   15    18  \n",
       "4      Australia   2023      1   15    19  \n",
       "..            ...   ...    ...  ...   ...  \n",
       "727            UK  2017      8   18    18  \n",
       "728           USA  2018      6   22    14  \n",
       "729        Canada  2019      4    5    17  \n",
       "730            UK  2020      2   29    20  \n",
       "731           USA  2020     11   15    15  \n",
       "\n",
       "[732 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sentimentdataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7344407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\sweta\n",
      "[nltk_data]     pandit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\sweta\n",
      "[nltk_data]     pandit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\sweta\n",
      "[nltk_data]     pandit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0   Enjoying a beautiful day at the park!        ...   \n",
      "1   Traffic was terrible this morning.           ...   \n",
      "2   Just finished an amazing workout! ðŸ’ª          ...   \n",
      "3   Excited about the upcoming weekend getaway!  ...   \n",
      "4   Trying out a new recipe for dinner tonight.  ...   \n",
      "\n",
      "                       cleaned_text  \n",
      "0       enjoying beautiful day park  \n",
      "1          traffic terrible morning  \n",
      "2          finished amazing workout  \n",
      "3  excited upcoming weekend getaway  \n",
      "4  trying new recipe dinner tonight  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('sentimentdataset.csv')\n",
    "\n",
    "# Apply the cleaning function to the 'Text' column\n",
    "df['cleaned_text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "# Print the first few rows of the DataFrame to verify the results\n",
    "print(df[['Text', 'cleaned_text']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389e5e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of TF-IDF Features: (732, 2154)\n",
      "\n",
      "First Few Rows of TF-IDF Features:\n",
      "   ablaze  abstract  abyss  academic  acceptance  accepts  accidentally  \\\n",
      "0     0.0       0.0    0.0       0.0         0.0      0.0           0.0   \n",
      "1     0.0       0.0    0.0       0.0         0.0      0.0           0.0   \n",
      "2     0.0       0.0    0.0       0.0         0.0      0.0           0.0   \n",
      "3     0.0       0.0    0.0       0.0         0.0      0.0           0.0   \n",
      "4     0.0       0.0    0.0       0.0         0.0      0.0           0.0   \n",
      "\n",
      "   accomplished  accomplishing  accomplishment  ...  year  yearbook  yearning  \\\n",
      "0           0.0            0.0             0.0  ...   0.0       0.0       0.0   \n",
      "1           0.0            0.0             0.0  ...   0.0       0.0       0.0   \n",
      "2           0.0            0.0             0.0  ...   0.0       0.0       0.0   \n",
      "3           0.0            0.0             0.0  ...   0.0       0.0       0.0   \n",
      "4           0.0            0.0             0.0  ...   0.0       0.0       0.0   \n",
      "\n",
      "   yet  york  young  zen  zero  zest  zestful  \n",
      "0  0.0   0.0    0.0  0.0   0.0   0.0      0.0  \n",
      "1  0.0   0.0    0.0  0.0   0.0   0.0      0.0  \n",
      "2  0.0   0.0    0.0  0.0   0.0   0.0      0.0  \n",
      "3  0.0   0.0    0.0  0.0   0.0   0.0      0.0  \n",
      "4  0.0   0.0    0.0  0.0   0.0   0.0      0.0  \n",
      "\n",
      "[5 rows x 2154 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the cleaned text data into TF-IDF features\n",
    "X = vectorizer.fit_transform(df['cleaned_text']).toarray()\n",
    "\n",
    "# Display the shape of the TF-IDF features\n",
    "print(\"\\nShape of TF-IDF Features:\", X.shape)\n",
    "\n",
    "# Display the first few rows of the TF-IDF features\n",
    "print(\"\\nFirst Few Rows of TF-IDF Features:\")\n",
    "print(pd.DataFrame(X, columns=vectorizer.get_feature_names_out()).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32862466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Few Labels:\n",
      "0     Positive  \n",
      "1     Negative  \n",
      "2     Positive  \n",
      "3     Positive  \n",
      "4     Neutral   \n",
      "Name: Sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Extract labels (sentiment) from the DataFrame\n",
    "y = df['Sentiment']  # Assuming the sentiment labels are in the 'Sentiment' column\n",
    "\n",
    "# Display the first few labels\n",
    "print(\"\\nFirst Few Labels:\")\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89f9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad980d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (585, 2154)\n",
      "X_test shape: (147, 2154)\n",
      "y_train shape: (585,)\n",
      "y_test shape: (147,)\n"
     ]
    }
   ],
   "source": [
    "# Display the shapes of the training and testing sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae1e53f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Parameters:\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "# Display the model's parameters\n",
    "print(\"\\nModel Parameters:\")\n",
    "print(model.get_params())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77185128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b13ff0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment labels for the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b50012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.11564625850340136\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "         Acceptance          0.00      0.00      0.00         2\n",
      "           Admiration        0.00      0.00      0.00         1\n",
      "        Admiration           0.00      0.00      0.00         1\n",
      "         Affection           0.00      0.00      0.00         1\n",
      "      Ambivalence            0.00      0.00      0.00         1\n",
      "         Anger               0.00      0.00      0.00         1\n",
      "        Anticipation         0.00      0.00      0.00         1\n",
      "        Arousal              0.00      0.00      0.00         3\n",
      "                  Awe        0.00      0.00      0.00         1\n",
      "         Awe                 0.00      0.00      0.00         1\n",
      "                  Bad        0.00      0.00      0.00         1\n",
      "             Betrayal        0.00      0.00      0.00         2\n",
      "        Betrayal             0.00      0.00      0.00         1\n",
      "         Bitter              0.00      0.00      0.00         1\n",
      "           Bitterness        0.00      0.00      0.00         1\n",
      "          Bittersweet        0.00      0.00      0.00         1\n",
      "              Boredom        0.00      0.00      0.00         1\n",
      "         Calmness            0.00      0.00      0.00         1\n",
      "          Captivation        0.00      0.00      0.00         1\n",
      "     Celestial Wonder        0.00      0.00      0.00         1\n",
      "             Colorful        0.00      0.00      0.00         1\n",
      "      Confusion              0.00      0.00      0.00         3\n",
      "           Connection        0.00      0.00      0.00         1\n",
      "        Contemplation        0.00      0.00      0.00         1\n",
      "          Contentment        0.00      0.00      0.00         3\n",
      "        Contentment          0.00      0.00      0.00         1\n",
      "         Coziness            0.00      0.00      0.00         1\n",
      "         Creativity          0.00      0.00      0.00         1\n",
      "            Curiosity        0.00      0.00      0.00         2\n",
      "          Curiosity          0.00      0.00      0.00         1\n",
      "      Curiosity              0.00      0.00      0.00         2\n",
      "           Desolation        0.00      0.00      0.00         1\n",
      "           Devastated        0.00      0.00      0.00         2\n",
      "              Disgust        0.00      0.00      0.00         1\n",
      "         Disgust             0.00      0.00      0.00         2\n",
      "        Elation              0.00      0.00      0.00         3\n",
      "             Elegance        0.00      0.00      0.00         1\n",
      "          Embarrassed        0.00      0.00      0.00         1\n",
      "       EmotionalStorm        0.00      0.00      0.00         1\n",
      "        Empowerment          0.00      0.00      0.00         1\n",
      "         Enjoyment           0.00      0.00      0.00         2\n",
      "           Enthusiasm        0.00      0.00      0.00         1\n",
      "              Envious        0.00      0.00      0.00         2\n",
      "  Envisioning History        0.00      0.00      0.00         1\n",
      "         Euphoria            0.00      0.00      0.00         1\n",
      "           Excitement        0.12      0.67      0.20         3\n",
      "         Excitement          0.00      0.00      0.00         3\n",
      "        Excitement           0.00      0.00      0.00         1\n",
      "         Fear                0.00      0.00      0.00         1\n",
      "              Fearful        0.00      0.00      0.00         1\n",
      "           Frustrated        0.00      0.00      0.00         1\n",
      "          Frustration        0.00      0.00      0.00         3\n",
      "         Fulfillment         0.00      0.00      0.00         2\n",
      "             Grateful        0.00      0.00      0.00         1\n",
      "      Grief                  0.00      0.00      0.00         1\n",
      "                Happy        0.00      0.00      0.00         6\n",
      "                 Hate        0.00      0.00      0.00         2\n",
      "           Heartbreak        0.00      0.00      0.00         2\n",
      "              Hopeful        1.00      1.00      1.00         1\n",
      "        InnerJourney         0.00      0.00      0.00         1\n",
      "        Inspiration          0.00      0.00      0.00         1\n",
      "             Inspired        0.00      0.00      0.00         1\n",
      "            Isolation        0.00      0.00      0.00         1\n",
      "          Jealousy           0.00      0.00      0.00         1\n",
      "                  Joy        0.14      0.88      0.25         8\n",
      "         Joy                 0.00      0.00      0.00         1\n",
      "        JoyfulReunion        0.00      0.00      0.00         1\n",
      "         Kind                0.00      0.00      0.00         1\n",
      "           Loneliness        1.00      1.00      1.00         1\n",
      "      Loneliness             0.00      0.00      0.00         1\n",
      "             LostLove        0.00      0.00      0.00         1\n",
      "      Melancholy             0.00      0.00      0.00         2\n",
      "       Miscalculation        0.00      0.00      0.00         1\n",
      "              Neutral        0.00      0.00      0.00         1\n",
      "        Nostalgia            0.00      0.00      0.00         1\n",
      "      Nostalgia              0.00      0.00      0.00         1\n",
      "      Numbness               0.00      0.00      0.00         1\n",
      "          Overwhelmed        0.00      0.00      0.00         1\n",
      "              Playful        0.00      0.00      0.00         2\n",
      "            Positive         0.08      0.67      0.14         9\n",
      "                Proud        0.00      0.00      0.00         1\n",
      "        Reflection           0.00      0.00      0.00         1\n",
      "       Regret                0.00      0.00      0.00         1\n",
      "           Resilience        0.00      0.00      0.00         1\n",
      "            Reverence        0.00      0.00      0.00         1\n",
      "         Sadness             0.00      0.00      0.00         2\n",
      "        Satisfaction         0.00      0.00      0.00         1\n",
      "             Serenity        0.00      0.00      0.00         2\n",
      "      Serenity               0.00      0.00      0.00         2\n",
      "             Solitude        0.00      0.00      0.00         1\n",
      "          Sorrow             0.00      0.00      0.00         1\n",
      "         Spark               0.00      0.00      0.00         1\n",
      "         Surprise            0.00      0.00      0.00         1\n",
      "        Thrill               0.00      0.00      0.00         1\n",
      "             Vibrancy        0.00      0.00      0.00         1\n",
      " Whispers of the Past        0.00      0.00      0.00         1\n",
      "                 Zest        0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.12       147\n",
      "             macro avg       0.02      0.04      0.03       147\n",
      "          weighted avg       0.03      0.12      0.04       147\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweta pandit\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\sweta pandit\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\sweta pandit\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30fb272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to a CSV file\n",
    "df.to_csv('cleaned_sentiment_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f40930a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to an SQLite database\n",
    "conn = sqlite3.connect('sentiment_analysis.db')\n",
    "df.to_sql('sentiments', conn, if_exists='replace', index=False)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99905d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'cleaned_sentiment_dataset.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = 'cleaned_sentiment_dataset.csv'\n",
    "\n",
    "if os.path.isfile(file_path):\n",
    "    print(f\"The file '{file_path}' has been created successfully.\")\n",
    "else:\n",
    "    print(f\"The file '{file_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db8d1eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'cleaned_sentiment_dataset.csv' is in the directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = '.'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "if file_path in files:\n",
    "    print(f\"The file '{file_path}' is in the directory.\")\n",
    "else:\n",
    "    print(f\"The file '{file_path}' is not found in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f4affbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'cleaned_sentiment_dataset.csv' has been read successfully.\n",
      "   Unnamed: 0.1  Unnamed: 0  \\\n",
      "0             0           0   \n",
      "1             1           1   \n",
      "2             2           2   \n",
      "3             3           3   \n",
      "4             4           4   \n",
      "\n",
      "                                                Text    Sentiment  \\\n",
      "0   Enjoying a beautiful day at the park!        ...   Positive     \n",
      "1   Traffic was terrible this morning.           ...   Negative     \n",
      "2   Just finished an amazing workout! ðŸ’ª          ...   Positive     \n",
      "3   Excited about the upcoming weekend getaway!  ...   Positive     \n",
      "4   Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
      "\n",
      "             Timestamp            User     Platform  \\\n",
      "0  2023-01-15 12:30:00   User123          Twitter     \n",
      "1  2023-01-15 08:45:00   CommuterX        Twitter     \n",
      "2  2023-01-15 15:45:00   FitnessFan      Instagram    \n",
      "3  2023-01-15 18:20:00   AdventureX       Facebook    \n",
      "4  2023-01-15 19:55:00   ChefCook        Instagram    \n",
      "\n",
      "                                     Hashtags  Retweets  Likes       Country  \\\n",
      "0   #Nature #Park                                  15.0   30.0     USA         \n",
      "1   #Traffic #Morning                               5.0   10.0     Canada      \n",
      "2   #Fitness #Workout                              20.0   40.0   USA           \n",
      "3   #Travel #Adventure                              8.0   15.0     UK          \n",
      "4   #Cooking #Food                                 12.0   25.0    Australia    \n",
      "\n",
      "   Year  Month  Day  Hour                      cleaned_text  \n",
      "0  2023      1   15    12       enjoying beautiful day park  \n",
      "1  2023      1   15     8          traffic terrible morning  \n",
      "2  2023      1   15    15          finished amazing workout  \n",
      "3  2023      1   15    18  excited upcoming weekend getaway  \n",
      "4  2023      1   15    19  trying new recipe dinner tonight  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df_check = pd.read_csv(file_path)\n",
    "    print(f\"The file '{file_path}' has been read successfully.\")\n",
    "    print(df_check.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file '{file_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78472e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sentimentdataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b4578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
